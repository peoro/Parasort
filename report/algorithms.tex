\section{Objectives, algorithms and assumptions}
\label{assumptions}
In the first part of our work we put a lot of efforts in deeply understanding the state of the art of parallel sorting algorithms. The first thing we noticed is the lack of an unified theory that asserts whether an algorithm is better than another; this is an obvious consequence of the absence of realistic cost models for parallel algorithms and architectures (e.g. both $PRAM$ and the more refined $LogP$ are too superficials to be considered significant. Second thing, the literature is rich of parallel algorithms conceived and engineered for \textit{specific machines}, so to be able to exploit peculiar features of the machine itself, e.g. the structure of the interconnection network. Just as examples we can cite~\cite{CSPA, CSPA2}, but with some googling we can really find a lot of papers or report on these themes. This approach is not obviously good because of at least two facts: the high designing complexity and the non-portability. Hence, in our project, even if the practical performance analysis will be done on a specific architecture, the parallel algorithms will \textit{not} be designed for that specific machine. In some sense, our approach is more close to~\cite{NPSA}.

We list the sorting algorithms that we are going to parallelize, all of them based on the Divide$\&$Conquer paradigm. Notice that, in general, the parallelization consist of ''exporting'' either the divide or the conquer phase to the ''process level''. Just as an example, in parallel Mergesort the distribution phase will be made between the processes of the parallel algorithm. 
\begin{enumerate}
\item Mergesort
\item K-Way Mergesort
\item Load-Balance Mergesort (and some variants) 
\item Quicksort
\item Bitonicsort
\item Bucketsort
\item Samplesort
\end{enumerate}
Examples of parallelization of $some$ of these algorithms can be found in literature. For instance, it is easy to find parallel versions of Mergesort, Quicksort, Samplesort, Bitonicsort. On the other hand, we notice the lack (or at least the poor availability) of informations regarding K-Way Mergesort and Load-Balance Mergesort. In any case, as we will better explain later, we re-implemented all the algorithms. Indeed, all the MPI implementations we found were really poor in terms of quality of code and do not take into account the possibility of having to sort large data sets, a problem which requires a specific analysis. Once implemented, we will compare the performance of these algorithms in terms of their efficiency, scalability and completion time, by playing on some degrees of freedom like the parallelism degree and the size of the data sets.  
 
\input{assumptions}
