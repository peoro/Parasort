\label{DAL}
Our algorithms must be able to handle big data set to some extent, for example we can easily think of a scenario where each core has to sort half a gigabyte of data, and we are using 256 cores; the total memory used, among the whole system, is 128 gigabytes of data.
At least the first phases of any algorithm (before data is fully distributed), and the last phases (while data is getting gathered) will have for sure to support datasets that cannot fit in main memory, and will be forced to run their computations on some files allocated in secondary memory.
This is a big issue, since it would force us to explicitly re-design any algorithm in order to make it handle both the mediums data could be stored in.
In order to limit the complexity of algorithms we decided to separate medium handling from actual algorithm code, thus creating a new abstraction layer in our application model.

At this purpose we decided to add a further abstraction layer below the actual Sorting Framework: we introduced a new data structure called \texttt{Data} that represents a dataset independently on its form or position: it can represent both an array allocated on principal memory or a file allocated on the hard disk, and it has been thought to be able to represent even other kind of data representation (eg: a compressed dataset). Both Sorting Algorithm and Sorting Framework, which are at a higher abstraction level, do not need to know about how or where \texttt{Data} is allocated, and use it in a transparent way.
We needed to write some kind of run-time support for the Sorting levels, which is logically placed just below the Sorting Framework: since we want to save the algorithm from actually care about how Data is allocated, Sorting Algorithm needs to use some functions that will take care of it exposing a data-independent signature. Our rule of thumb is that all and only the functions logically placed at this abstraction level are the only ones actually working with a \texttt{Data} object (ie: accessing its field directly or indirectly) and vice-versa.
Functions at the DAL level will one (or more) \texttt{Data} object, will see whether it's allocated on primary or secondary memory, and according to this they'll run some data-dependent codes, optimized for the medium where the \texttt{Data} object is allocated.

\subsection*{Data Abstraction Layer - API}
\texttt{Data} is the key datatype of the DAL. Designing a Sorting Algorithm does not require to know the internal structure of \texttt{Data}, but just what it represent: a sequence of elements. Indeed, all the processes of a Sorting Algorithm declare their own \texttt{Data} (eventually more than one, depending on the specific algorithm) which will be passed as parameter to the functions of the DAL's API. To initialize and destroy a \texttt{Data} we provide two specific functions:
\begin{lstlisting}
void DAL_init( Data *data )
void DAL_destroy( Data *data )
\end{lstlisting}
Obviously, processes of a Sorting Algorithm have to collaborate somehow: DAL provides them with a set of communication primitives. The most important ones are:
\begin{lstlisting}
void DAL_send( Data *data, int dest )
void DAL_receive( Data *data, long size, int source )
void DAL_scatter( Data *data, long size, int root )
void DAL_gather( Data *data, long size, int root )
void DAL_alltoallv( Data *sendData, long *sendSizes, long *sdispls, long *recvSizes, long *rdispls )
\end{lstlisting}
The semantic of these functions is the same of the ones provided by MPI having analogous name. Notice that all the processes of the Sorting Algorithm, except the one of Rank $0$, starts with an empty \texttt{Data}, as consequence of the choice of starting with a centralized data set (see~\ref{assumptions}). Anyway, a very simple way to initialize \texttt{Data} to all processes is to perform an initial scatter of the data set.

There are other functions that need to access \texttt{Data}, but with respect to the ones described above are more related to the Sorting Algorithms.  Just as example, think to a function $merge$, that takes in input at least two \texttt{Data} and returns another \texttt{Data} which represents the sorted concatenation of the input sequences. This function has to be implemented for specific algorithms (like $mergesort$ an $k-way mergesort$), but on the other hand, it needs to access the fields of \texttt{Data} to be implemented. We look at these type of functions as they would belong to the run-time support of Sorting Algorithms: that is, they are logically placed at the DAL level even if they are implemented outside the DAL library. It is like we had an additional level between the DAL and Sorting Framework, eventually called ''Sorting DAL'', where all the functions that are related to the support of the algorithms, but need to access the DAL, should be logically placed. 

\subsection*{Data Abstraction Layer - Implementation}
The type \texttt{Data} is implemented as follows:
\begin{lstlisting}
typedef struct
{
	enum Medium {
		NoMedium = 0,
		File = 1,
		Array = 2
	} medium;
	
	union
	{
		struct
		{
			int *data;
			long size;
		} array;
		struct
		{
			FILE *handle;
			char name[128];
		} file;
	};
	
} Data;
\end{lstlisting}
Depending on the value of $medium$, a \texttt{Data} represents a sequence of elements that are currently being stored in main memory ($Array$), on disk ($File$) or not initialized yet ($NoMedium$). At this purpose, an $union$ is used to reflect the fact that \texttt{Data} is a generic type.  As we have already said, a \texttt{Data} is normally accessed and manipulated by the functions of the API illustrated in the previous section. However, looking at the code, it is likely to encounter other functions (that substantially act like macros) which address specific purposes: 
\begin{lstlisting}
bool DAL_allocArray( Data *data, int size )
bool DAL_reallocArray ( Data *data, int size )
\end{lstlisting}
The semantic of these functions follow directly from their signature. The $DAL\_allocArray$ tries to allocate a block of $size$ integers in main memory. The $DAL\_reallocArray$ simply resize a \texttt{Data} preserving its contents; notice that a call to this function may cause the change of the medium. 

To give an idea of how we implemented the DAL's API functions (and so, how a \texttt{Data} is manipulated), we focus on the specific case of the primitive $send$. For implementing all other primitives we adopted a very similar approach; anyway, notice that other functions, like the $AllToAll$, are really trickier than the $send$. 
\begin{lstlisting}
void DAL_send( ..., Data *data, int dest )
{
	switch( data->medium ) {
		case File: {
			Data *buf;
			while ( buf = DAL_readNextBlock( data ):
				DAL_send( buf )
			break;
		}
		case Array: {
			MPI_Send( data->array.data, data->array.size, MPI_INT, dest, 0, MPI_COMM_WORLD );
			break;
		}
		default:
			UNSUPPORTED_DATA( data );
	}
}
\end{lstlisting}
Depending on the medium where the block of data is stored, different actions are performed. If the medium is the main memory (''Array''), than a simple call to a primitive of the lower level is enaugh (ie MPI$\_$Send). On the other hand, the case ''File'', that is elements are allocated on disk, is a little bit trickier and subject of potential optimizations that we will discuss in~\ref{conclusion}. Here we present a simple possible implementation. The DAL is provided with two new functions: 
\begin{lstlisting}
Data* DAL_readNextBlock(  Data* )
Data* DAL_allocBuffer( int size )
\end{lstlisting}
$DAL\_readNextBlock( Data* )$ takes a \texttt{Data} whose medium is of type ''File'' and returns a new \texttt{Data} which contains a sequence of elements buffered in main memory. The cursor of the file from which elements have been taken is obviously shifted, so a subsequent call to the function with the same argument will return the following block of data that fits the memory. $DAL\_allocBuffer( int\ size )$ is called internally by $DAL\_readNextBlock( Data* )$. It aims at allocating in main memory a block of \textit{at most} $size$ elements; if $size$ elements are too many to be allocated in memory, than some strategies can be adopted to find a new $size$, which is as close as possible to the one passed as argument, such that the returned \texttt{Data} will contain a set of elements stored in main memory. 
By looping on $DAL\_readNextBlock( Data* )$ we can split the initial \texttt{Data}, that could not fit the main memory, into two or more blocks that instead fits it; then, each of this block will be sent in succession.
This was the macroscopic behaviour of a simple function like $DAL\_send$. Unfortunately, things get even more complicated if we go into the details of the implementation. Indeed, a subtle issue arises for implementing $DAL\_readNextBlock( )$. Let's think to the fact that after a call to $DAL\_allocBuffer( int\ size )$, sender and receiver may end up with buffers of different $size$ in their main memory. In this case, the $MPI\_Send$ and the corresponding $MPI\_Recv$ get in some sense decoupled. The real issue is due to a receiver which allocs a buffer smaller than the one of the sender, since in MPI a $MPI\_Send$ has to match exactly one $MPI\_Recv$~\cite{MPI}. Further, a potential probelm regarding the memory usage may arise: a process may allocate a lot of memory which is potentially useless, because of the problem stated above, and contextually take out available memory to the other processes that runs on the same machine. Solution: static vs dynamic TODO.
%Send e receive debbono essere simmetriche, quindi entrambe le parti debbano sapere la dimensione altrui. Abbiamo pensato a queste due soluzioni: static: ogni processo ha preallocato un buffer di dimensione fissa per fare le send..contiamo sul fatto che fare 10 send da 50MB o 1 send da 500 cambi relativamente poco in termini di tempo xkè quando si manda tanta roba i tsetup s dovrebbero pagare relativamente meno. L'altra possibilità era una soluzione dinamica, in cui i processi partner fanno un handshake per stabilire quanto inviare; c sarebbero vari problemi..RTT addizionali, possibilità di incongruenze ecc !!! 
