\label{DAL}
Our algorithms must be able to handle big data set to some extent, for example we can easily think of a scenario where each core has to sort half a gigabyte of data, and we are using 256 cores; the total memory used, among the whole system, is 128 gigabytes of data.
At least the first phases of any algorithm (before data is fully distributed), and the last phases (while data is getting gathered) will have for sure to support datasets that cannot fit in main memory, and will be forced to run their computations on some files allocated in secondary memory.
This is a big issue, since it would force us to explicitly re-design any algorithm in order to make it handle both the mediums data could be stored in.
In order to limit the complexity of algorithms we decided to separate medium handling from actual algorithm code, thus creating a new abstraction layer in our application model.

At this purpose we decided to add a further abstraction layer below the actual Sorting Framework: we introduced a new data structure called \texttt{Data} that represents a dataset independently on its form or position: it can represent both an array allocated on principal memory or a file allocated on the hard disk, and it has been thought to be able to represent even other kind of data representation (eg: a compressed dataset). Both Sorting Algorithm and Sorting Framework, which are at a higher abstraction level, do not need to know about how or where \texttt{Data} is allocated, and use it in a transparent way.
We needed to write some kind of run-time support for the Sorting levels, which is logically placed just below the Sorting Framework: since we want to save the algorithm from actually care about how Data is allocated, Sorting Algorithm needs to use some functions that will take care of it exposing a data-independent signature. Our rule of thumb is that all and only the functions logically placed at this abstraction level are the only ones actually working with a \texttt{Data} object (ie: accessing its field directly or indirectly) and vice-versa.
Functions at the DAL level will one (or more) \texttt{Data} object, will see whether it's allocated on primary or secondary memory, and according to this they'll run some data-dependent codes, optimized for the medium where the \texttt{Data} object is allocated.

\subsection*{Data Abstraction Layer - API}
\texttt{Data} is the key datatype of the DAL. Designing a Sorting Algorithm does not require to know the internal structure of \texttt{Data}, but just what it represent: a sequence of elements. Indeed, all the processes of a Sorting Algorithm declare their own \texttt{Data} (eventually more than one, depending on the specific algorithm) which will be passed as parameter to the functions of the DAL's API. To initialize and destroy a \texttt{Data} we provide two specific functions:
\begin{lstlisting}
void DAL_init( Data *data )
void DAL_destroy( Data *data )
\end{lstlisting}
Obviously, processes of a Sorting Algorithm have to collaborate somehow: DAL provides them with a set of communication primitives. The most important ones are:
\begin{lstlisting}
void DAL_send( Data *data, int dest )
void DAL_receive( Data *data, long size, int source )
void DAL_scatter( Data *data, long size, int root )
void DAL_gather( Data *data, long size, int root )
void DAL_alltoallv( Data *sendData, long *sendSizes, long *sdispls, long *recvSizes, long *rdispls )
\end{lstlisting}
The semantic of these functions is the same of the ones provided by MPI having analogous name. Notice that all the processes of the Sorting Algorithm, except the one of Rank $0$, starts with an empty \texttt{Data}, as consequence of the choice of starting with a centralized data set (see~\ref{assumptions}). Anyway, a very simple way to initialize \texttt{Data} to all processes is to perform an initial scatter of the data set.

There are other functions that need to access \texttt{Data}, but with respect to the ones described above are more related to the Sorting Algorithms.  Just as example, think to a function $merge$, that takes in input at least two \texttt{Data} and returns another \texttt{Data} which represents the sorted concatenation of the input sequences. This function has to be implemented for specific algorithms (like $mergesort$ an $k-way mergesort$), but on the other hand, it needs to access the fields of \texttt{Data} to be implemented. We look at these type of functions as they would belong to the run-time support of Sorting Algorithms: that is, they are logically placed at the DAL level even if they are implemented outside the DAL library. It is like we had an additional level between the DAL and Sorting Framework, eventually called ''Sorting DAL'', where all the functions that are related to the support of the algorithms, but need to access the DAL, should be logically placed. 

\subsection*{Data Abstraction Layer - Implementation}
\label{DAL-impl}
The type \texttt{Data} is implemented as follows:
\begin{lstlisting}
typedef struct
{
	enum Medium {
		NoMedium = 0,
		File = 1,
		Array = 2
	} medium;
	
	union
	{
		struct
		{
			int *data;
			long size;
		} array;
		struct
		{
			FILE *handle;
			char name[128];
		} file;
	};
	
} Data;
\end{lstlisting}
Depending on the value of $medium$, a \texttt{Data} represents a sequence of elements that are currently being stored in main memory ($Array$), on disk ($File$) or not initialized yet ($NoMedium$). At this purpose, an $union$ is used to reflect the fact that \texttt{Data} is a generic type.  As we have already said, a \texttt{Data} is normally accessed and manipulated by the functions of the API illustrated in the previous section. However, looking at the code, it is likely to encounter other functions (that substantially act like macros) which address specific purposes: 
\begin{lstlisting}
bool DAL_allocArray( Data *data, int size )
bool DAL_reallocArray ( Data *data, int size )
\end{lstlisting}
The semantic of these functions follow directly from their signature. The $DAL\_allocArray$ tries to allocate a block of $size$ integers in main memory. The $DAL\_reallocArray$ simply resize a \texttt{Data} preserving its contents; notice that a call to this function may cause the change of the medium. 

To give an idea of how we implemented the DAL's API functions (and so, how a \texttt{Data} is manipulated), we focus on the specific case of the primitive $send$. For implementing all other primitives we adopted a very similar approach; anyway, notice that other functions, like the $AllToAll$, are really trickier than the $send$. 
\begin{lstlisting}
void DAL_send( ..., Data *data, int dest )
{
	switch( data->medium ) {
		case File: {
			Data *buf;
			while ( buf = DAL_readNextBlock( data ):
				DAL_send( buf )
			break;
		}
		case Array: {
			MPI_Send( data->array.data, data->array.size, MPI_INT, dest, 0, MPI_COMM_WORLD );
			break;
		}
		default:
			UNSUPPORTED_DATA( data );
	}
}
\end{lstlisting}
Depending on the medium where the block of data is stored, different actions are performed. If the medium is the main memory (''Array''), than a simple call to a primitive of the lower level is enaugh (ie MPI$\_$Send). On the other hand, the case ''File'', that is elements are allocated on disk, is a little bit trickier and subject of potential optimizations that we will discuss in~\ref{conclusion}. Here we present a simple possible implementation. Let's first introduce two essential functions: 
\begin{lstlisting}
Data* DAL_readNextBlock(  Data* )
Data* DAL_allocBuffer( int size )
\end{lstlisting}
$DAL\_readNextBlock( Data* )$ takes a \texttt{Data} whose medium is of type ''File'' and returns a new \texttt{Data} which contains a sequence of elements buffered in main memory. The cursor of the file from which elements have been taken is obviously shifted, so a subsequent call to the function with the same argument will return the following block of data that fits the memory. $DAL\_allocBuffer( int\ size )$ is called internally by $DAL\_readNextBlock( Data* )$. It aims at allocating in main memory a block of \textit{at most} $size$ elements; if $size$ elements are too many to be allocated in memory, than some strategies can be adopted to find a new $size$, which is as close as possible to the one passed as argument, such that the returned \texttt{Data} will contain a set of elements stored in main memory. 
By looping on $DAL\_readNextBlock( Data* )$ we can split the initial \texttt{Data}, that could not fit the main memory, into two or more blocks that instead fits it; then, each of this block will be sent in succession.
This was the macroscopic behaviour of a simple function like $DAL\_send$. Unfortunately, things get even more complicated if we go into the details of the implementation. Indeed, a subtle issue arises for implementing $DAL\_readNextBlock( )$. Let's think to the fact that after a call to $DAL\_allocBuffer( int\ size )$, sender and receiver may end up with buffers of different $size$ in their main memory. In this case, the $MPI\_Send$ and the corresponding $MPI\_Recv$ get in some sense decoupled, since the sender may be able to send $X$ elements at a time, while the receiver may be able to receive $Y$ elements at a time, with $X \neq Y$ in general. In particular, the real issue arises once the receiver allocates a buffer smaller than the one allocated by the sender (that is $Y < X$): in MPI a $MPI\_Send$ matches exactly one $MPI\_Recv$ (see the MPI standard~\cite{MPI}), thereby the asymmetry between the size of the two buffers will cause unacceptable loss of data. As if that were not enough, another potential probelm regarding the memory usage would arise: because of the ''buffer-asymmetry problem'' stated above, a process may allocate a lot of potentially useless memory, subtracting it to the other processes that are being executed on the same machine. We propose two possible solutions for this problem: 
\begin{itemize}
\item \textbf{Static buffer size.} The size $X$ of the buffer is \textit{pre-established} and a communication-buffer is \textit{pre-allocated} to all the processes. The hypothetical optimal value of $X$ can be determined by applying some heuristics regarding the average available memory of a node, the number of cores, the size of the data set and so on. Hence, a part of the main memory of a node get waste in favour of the communication run-time support, in sense that it can not be used for performing the sorting. 
\item \textbf{Dynamic buffer size.} Another possibility is to let sender and receiver accord to the size of the buffer by exchanging some preliminary messages before starting the real communication. For instance, sender may ask receiver to allocate a buffer of $X$ size. Receiver may answer ''Yes, I can'' or ''No, I can't''. Receiving a negative answer, sender may try with proposing a different size, leading to a potential ''ping-pong'' effect. This undesirable effect is in some sense stochastic, and we do not know a-priori the whole overhead that may cause. However, we can avoide this problem by letting the receiver to answer with a message like this: ''No, i can't, but i was able to allocate space for $Y$ elements. So, send me $Y$ elements''. In this case, only two preliminary messages have been exchanged between sender and receiver; then, the real send can be performed by sending a block of $Y$ elements at a time. Notice that with this solution some overhead comes from the dynamic memory allocation. 
\end{itemize}
We chose the static approach. On the one hand, we avoid the overhead due to both the preliminary phase of ''handshaking'' and the dynamic memory allocation (that could be very significative in case of high values of the size) which characterize the dynamic solution. On the other hand, it is simpler to be implemented with respect to the dynamic approach, which should be specifically designed for each communication primitive ($DAL\_\lbrace$send, scatter, alltoall, ...$\rbrace$). However, as future work, we propose to implement also the dynamic solution in order to have a practical comparison between these two approaches. Finally, notice that both solutions require $DAL\_send$ to be implemented with a loop of $MPI\_Send$ when working on files. We think that \textit{in general}, with the static solution, being the buffer size pre-established, we need a greater number of $MPI\_Send$ with respect to the dynamic solution in order to perform the same $DAL\_send$. Anyway, we think also that for sending megabytes (or greater) of data, the overhead due to the setup of more than one communication is negligible with respect to the time taken by the transfer of the data. We will investigate these aspects in chapter~\ref{MPI-cost-model}, where we will conclude that our assumptions actually hold.

