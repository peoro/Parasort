\section{Performance evaluation}

\subsection{Performance measurement approach}
\label{test-env}
There are two different common approaches to evaluate performance: profiling and benchmarking.

Profiling is used to measure the performance of a given application by adding time measuring and logging code. After the execution has been completed, one can retrace and check how much time had been spent in which function call. This is useful for application developers to find potential bottlenecks and inefficient implemented functions.

Benchmarking addresses mainly the levels underneath the application level, for instance the performance
of the MPI-implementation, the performance of the network and devices. Most interesting results are latencies and bandwidths and their course for various packet or messages sizes. Benchmarking is done by performing numerous iterations of computation and/or communication and measuring the elapsed time.

We used the latter technique to measure the performance of MPI on our test environments, and a combination of both benchmarking and profiling techniques to measure the performance of the different sorting algorithms.

\subsection{Test environment}
In this section we briefly discuss the architecture of our test environments. 

\paragraph{Pianosa}
\textit{Pianosa} is an old cluster of 30 nodes located at the Department of Computer Science, University of Pisa. Nodes are Intel Pentium III 800 Mhz with a L1 cache of 512KB. The primary memory is of size 1 GB. The operative system is an old distribution of Linux, the Fedora Core 1. On average, the available space on disk in each machine is roughly 15 GB; this capacity obviously limits our tests to data sets of relatively small size. The interconnection network between nodes is a Fast Ethernet, so the MPI support is built on top of the stack TCP/IP. The main weakness of the whole architecture is that the interconnection network is based on a hub: the conflicts at network level becomes not negligible, specially for messages of significative size. This is not just an hyphotesis, but something concrete which has been experienced by the group of Parallel Architectures when attempting to define a cost model of MPI. Anyway, even if the architecture has a lot of limitations that can \textit{significantly} affect the performance, Pianosa is a good starting point for testing our algorithms (even from a functional point of view) and to conjecture which architectual characteristics could impair the performance of a Parallel Sorting Algorithm.

\paragraph{PCM}
\label{PCM}
The second target architecture is a cluster of Intel Xeon X5670 (2,93 Ghz) machines located at the Department of Physics, University of Pisa. Each node has 12 cores splitted in 2 chips (6 cores/chip) and each core allows the execution of two simultaneous threads. Each core has both a L1 cache of 64KB (32 KB dedicated to instructions, 32KB to datas) and L2 cache of 256KB. Each chip has also a L3 cache of 12MB, thus shared by 6 cores. The primary memory is of size 48GB. The disk subsystem is 1x1000 GB SATA II, 7200 RPM, surely faster than the one of Pianosa. The operative system is SUSE Linux Enterprise Server 11. Nodes are interconnected by means of Infiniband. We will use \textit{mvapich2}, a special version of \textit{mpich2} that supports Infiniband. It is clear that the comparison between the old Pianosa and this cluster is unfair: the newer powerful hardware of this architecture will play a key role for the performance of our Sorting Algorithms, both from a quantitative (absolute time completion) and qualitative (scalability) point of view. We will detail all these aspects in the following sections.

\subsection{Cost Model of MPI}
\label{MPI-cost-model}
\paragraph{The cost of sending a message: the T$\_$send model}
We are interested in evaluating the cost of communications in the test environments because it can help us in understanding the performance behaviour of the Sorting Algorithms. Indeed, depending on both the specific architecture and the algorithm, communications may significatively impact the completion time (and so even the scalability) of the algorithm itself. In a simplified view, the cost of sending a message between two tasks located on different processors can be represented by two parameters: the message startup time $T_{setup}$, which is the time required to initiate the communication, and the transmission time $T_{trasm}$ per word, which is mainly determined by the physical bandwidth of the communication channel linking the source and destination processors~\cite{VANN}. In this model, the time required to send a message of size $L$ (that is, for performing a $DAL\_send$ of size $L$) words is then
\[
T_{send}(L) = T_{setup} + L \times T_{trasm}
\]
While accurate for some algorithms and on very simple architectures, this model generally breaks down due to a lot of factors: for instance, the communication pattern of the application, the bandwith of the interconnection network and, more in general, due to the complexity of the parallel architectures. Despite its weakness, for practical reasons we will refer to the $T\_send$-model to explain the results we achieved.  
The following experimental analysis will try to estimate $T\_send$ on our test architectures by varying the size of the messages and by changing the communication pattern (we will perform the so called ''bisection test'', whichin there are two sets of processes, with same cardinality, that exchange messages each other). 

In order to benchmark the performance of the two environments, we decided to use the \textit{Perftest} benchmark suite.

The perftest-package is provided along with the MPI-implementation MPICH, although it can be used in combination with any MPI-implementation. The package contains a few tools to measure the performance of a message passing environment. The two major programs are \textit{mpptest} for measuring point-to-point communication and \textit{goptest} for measuring collective communication. In addition to the classic ping-pong test, mpptest can measure performance with many participating processes (exposing contention and scalability problems) and can adaptively choose the message sizes in order to isolate sudden changes in performance. 

The following test were performed:
\begin{itemize}
	\item Round-trip times between 2 nodes using blocking methods
	\item Blocking bisection times involving from 4 to $n$ nodes
	\item Broadcast times involving from 2 to $n$ nodes
\end{itemize}
In the bisection test the complete system is logically divided into two subsystems and the aggregated bandwidth and latency between the two subsystems is measured. An example of this is splitting the system in two vertically and letting each node in the left half communicate with a node in the right part on a one-to-one basis.

\paragraph{Optimum size of the DAL's communication buffer}
As we said in section~\ref{DAL-impl} we are interested in estimating the \textit{optimum size} for the communication buffer of the DAL layer. In particular, let's assume that we are performing a $DAL\_send$ of $L$ words. We may have to send these $L$ words either by performing a single $MPI\_Send$ or, more in general, by performing $X$ $MPI\_Send$. The latter situation is not unusual in our framework due to the structure of the DAL layer. Indeed, from section~\ref{DAL-impl}, we remind that each process has its own buffer dedicated to MPI-communications, and this buffer is of fixed size, let's say $K$ words. Since in general $K < L$, assuming without loss of generality that $L = K \times X$, the cost of sending $L$ words is equal to the cost of performing $X$ $MPI\_Send$ each of size $K$. For each architecture, we have to choose the best value of $K$: indeed, a low value may negatively impact on $T\_send$ (because of the setup cost of each $MPI\_Send$); a high value could be useless because a too large communication buffer may not give any benefit, rather it may only constitue a waste of important memory. In order to find the optimal value of $K$, namely the size of the ''static'' communication buffer, we developed a set of utilites (program and script that goes under the name of \textit{Tsetup}) that practically estimates $T\_send$ as function of $L$ and $X$. 

\paragraph{MPI on Pianosa}
\label{test-env-pianosa}
Figures~\ref{pianosa-mpi-1},~\ref{pianosa-mpi-2},~\ref{pianosa-mpi-3} show the cost of $T\_send$ obtained on Pianosa. We will refer to these Figures in the following, when we will explain the performance analisys of the algorithms.

\begin{figure}[p]
	\centering
  	\subfloat[Messages of length from 0 to 32 bytes (stride 4 bytes).]{\label{tsend_32}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/tsend_4}}  
	\hspace*{20pt}
  	\subfloat[Messages of length from 0 to 1024 bytes (stride 32 bytes).]{\label{tsend_32}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/tsend_32}}  
	\caption{Performance of blocking send between two processes.}
	\label{pianosa-mpi-1}
	
	\centering
  	\subfloat[Messages of length from 0 to 32768 bytes (stride $2^i$ bytes).]{\label{tsend_logscale}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/tsend_logscale}}  
	\hspace*{20pt}
  	\subfloat[Bisection test with 4, 8 and 16 processors.]{\label{bisect_logscale}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/bisect_logscale}}  
	\caption{Performance of blocking send and bisection test.}
	\label{pianosa-mpi-2}	
	
	\centering {\label{bcast}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/bcast}}  
	\caption{Performance of broadcast with 4, 8 and 16 processors.}
	\label{pianosa-mpi-3}
\end{figure}

Now we focus on the size of the communication buffer. Table~\ref{tsetup-impact} shows the cost in seconds of DAL$\_\lbrace send, scatter, alltoall \rbrace$ for a message of size 32MB. The $X$ row of the table show the case in which the original message has been splitted in $X$ $MPI\_\lbrace send, scatter, alltoall \rbrace$, with $X \in \lbrace 2^i : i = 0...18 \rbrace$. TODO: need comment
\begin{table}[h]
\begin{center}
\begin{tabular}{|c|c|c|c|c|}\hline
\hline
$\sharp$ MPI calls & MPI size ($\frac{L}{\sharp Sends}$ bytes)  & $T\_send$   & $T\_scatter$  & $T\_alltoall$      \\\hline\hline
1 & 33554432 & 3.2890 & 3.2557 & 5.4890 \\\hline
2 & 16777216 & 3.2845 & 3.2548 & 5.4769 \\\hline
4 & 8388608 & 3.2847 & 3.2542 & 5.5114 \\\hline
8 & 4194304 & 3.2852 & 3.2549 & 5.4967 \\\hline
16 & 2097152 & 3.2862 & 3.2560 & 5.5235 \\\hline
32 & 1048576 & 3.2867 & 3.2577 & 21.21224 \\\hline
64 & 524288 & 3.2919 & 3.2892 & 26.26160 \\\hline
128 & 262144 & 3.3007 & 3.2829 & 32.31925 \\\hline
256 & 131072 & 3.2847 & 3.2531 & 41.40975 \\\hline
512 & 65536 & 3.2891 & 3.2542 & 72.71889 \\\hline
1024 & 32768 & 3.2854 & 3.2544 & 114.113511 \\\hline
2048 & 16384 & 3.2853 & 3.2553 & 265.265055 \\\hline
4096 & 8192 & 3.2859 & 3.2580 & 15.15112 \\\hline
8192 & 4096 & 3.2868 & 3.2645 & 20.20080 \\\hline
16384 & 2048 & 3.2892 & 3.2790 & 48.47531 \\\hline
32768 & 1024 & 3.2935 & 3.3275 & 84.84000 \\\hline
65536 & 512 & 3.3221 & 4.4270 & 155.154831 \\\hline
131072 & 256 & 4.3956 & 6.6259 & 291.290647 \\\hline
262144 & 128 & 6.5900 & 15.15130 & 573.572866 \\\hline
\end{tabular}
\caption{DAL$\_\lbrace send, scatter, alltoall \rbrace$ cost for messages of size 32MB. }
\label{tsetup-impact}
\end{center}
\end{table}


\paragraph{MPI on PCM}

\input{alg_efficiency}
\input{performance}

\subsection{Comparing the results}