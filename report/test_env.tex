\section{Performance evaluation}

\subsection{Performance measurement approach}
\label{test-env}
There are two different common approaches to evaluate performance: profiling and benchmarking.

Profiling is used to measure the performance of a given application by adding time measuring and logging code. After the execution has been completed, one can retrace and check how much time had been spent in which function call. This is useful for application developers to find potential bottlenecks and inefficient implemented functions.

Benchmarking addresses mainly the levels underneath the application level, for instance the performance
of the MPI-implementation, the performance of the network and devices. Most interesting results are latencies and bandwidths and their course for various packet or messages sizes. Benchmarking is done by performing numerous iterations of computation and/or communication and measuring the elapsed time.

We used the latter technique to measure the performance of MPI on our test environments, and a combination of both benchmarking and profiling techniques to measure the performance of the different sorting algorithms.

\subsection{Test environment}
\subsubsection{Pianosa}
\subsubsection{''Physics cluster''}

\subsection{MPI cost model}
We are interested in evaluating the cost of communications in the test environments because it can help us in understanding the performance behaviour of the Sorting Algorithms. Indeed, depending on both the specific architecture and the algorithm, communications may significatively impact the completion time (and so even the scalability) of the algorithm itself. In a simplified view, the cost of sending a message between two tasks located on different processors can be represented by two parameters: the message startup time $T_{setup}$, which is the time required to initiate the communication, and the transmission time $T_{trasm}$ per word, which is determined by the physical bandwidth of the communication channel linking the source and destination processors~\cite{VANN}. In this model, the time required to send a message of size L words is then
\[
T_{send} = T_{setup} + L * T_{trasm}
\]
While accurate for some algorithms and on very simple architectures, this model can break down due to a lot of reasons: for instance, the communication pattern of the application, the bandwith of the interconnection network, and so on. The following analysis will try to estimate the cost of communications on our test architectures by varying the size of the messages and by changing the communication pattern (we will perform the so called ''bisection test'', whichin there are two sets of processes, with same cardinality, that exchange messages each other). 

In order to benchmark the performance of the two environments, we decided to use the \textit{Perftest} benchmark suite.

The perftest-package is provided along with the MPI-implementation MPICH, although it can be used in combination with any MPI-implementation. The package contains a few tools to measure the performance of a message passing environment. The two major programs are \textit{mpptest} for measuring point-to-point communication and \textit{goptest} for measuring collective communication. In addition to the classic ping-pong test, mpptest can measure performance with many participating processes (exposing contention and scalability problems) and can adaptively choose the message sizes in order to isolate sudden changes in performance. 

The following test were performed:
\begin{itemize}
	\item Round-trip times between 2 nodes using blocking methods
	\item Blocking bisection times involving from 4 to $n$ nodes
	\item Broadcast times involving from 2 to $n$ nodes
\end{itemize}
In the bisection test the complete system is logically divided into two subsystems and the aggregated bandwidth and latency between the two subsystems is measured. An example of this is splitting the system in two vertically and letting each node in the left half communicate with a node in the right part on a one-to-one basis.


\subsubsection*{Pianosa}
\label{test-env-pianosa}
Figures~\ref{pianosa-mpi-1},~\ref{pianosa-mpi-2},~\ref{pianosa-mpi-3} show the results obtained on Pianosa:

\begin{figure}[p]
	\centering
  	\subfloat[Messages of length from 0 to 32 bytes (stride 4 bytes).]{\label{tsend_32}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/tsend_4}}  
	\hspace*{20pt}
  	\subfloat[Messages of length from 0 to 1024 bytes (stride 32 bytes).]{\label{tsend_32}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/tsend_32}}  
	\caption{Performance of blocking send between two processes.}
	\label{pianosa-mpi-1}
	
	\centering
  	\subfloat[Messages of length from 0 to 32768 bytes (stride $2^i$ bytes).]{\label{tsend_logscale}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/tsend_logscale}}  
	\hspace*{20pt}
  	\subfloat[Bisection test with 4, 8 and 16 processors.]{\label{bisect_logscale}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/bisect_logscale}}  
	\caption{Performance of blocking send and bisection test.}
	\label{pianosa-mpi-2}	
	
	\centering {\label{bcast}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/bcast}}  
	\caption{Performance of broadcast with 4, 8 and 16 processors.}
	\label{pianosa-mpi-3}
\end{figure}

\input{performance}

\subsection{Algorithms Efficiency}
In this section, we are going to analyze the theoretical Algorithm Efficiency already defined in~\ref{terminology}. For the sake of commodity, we report here the definition. Given
\begin{itemize}
\item $P$, the number of real processors of our machine; 
\item $m$, the number of steps of the algorithm;
\item $X_i$, a random variable which counts the number of real processors that contribute to the calculation during the step $i$.
\end{itemize}
We defined the Algorithm Efficiency $\varphi$ as:
\begin{center}
$\varphi = \frac{\sum_{i=0}^m E[X_i]}{P \times m} $
\end{center}
TODO: for each algorithm, give the efficiency.

\subsection{Comparing the results}