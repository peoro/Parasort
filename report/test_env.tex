\section{Performance evaluation}

\subsection{Performance measurement approach}
\label{test-env}
There are two different common approaches to evaluate performance: profiling and benchmarking.

Profiling is used to measure the performance of a given application by adding time measuring and logging code. After the execution has been completed, one can retrace and check how much time had been spent in which function call. This is useful for application developers to find potential bottlenecks and inefficient implemented functions.

Benchmarking addresses mainly the levels underneath the application level, for instance the performance
of the MPI-implementation, the performance of the network and devices. Most interesting results are latencies and bandwidths and their course for various packet or messages sizes. Benchmarking is done by performing numerous iterations of computation and/or communication and measuring the elapsed time.

We used the latter technique to measure the performance of MPI on our test environments, and a combination of both benchmarking and profiling techniques to measure the performance of the different sorting algorithms.

\subsection{Test environment}
\paragraph{Pianosa}
\paragraph{''Physics cluster''}

\subsection{Cost Model of MPI}
\label{MPI-cost-model}
\paragraph{The cost of sending a message: the T$\_$send model}
We are interested in evaluating the cost of communications in the test environments because it can help us in understanding the performance behaviour of the Sorting Algorithms. Indeed, depending on both the specific architecture and the algorithm, communications may significatively impact the completion time (and so even the scalability) of the algorithm itself. In a simplified view, the cost of sending a message between two tasks located on different processors can be represented by two parameters: the message startup time $T_{setup}$, which is the time required to initiate the communication, and the transmission time $T_{trasm}$ per word, which is mainly determined by the physical bandwidth of the communication channel linking the source and destination processors~\cite{VANN}. In this model, the time required to send a message of size $L$ (that is, for performing a $DAL\_send$ of size $L$) words is then
\[
T_{send}(L) = T_{setup} + L \times T_{trasm}
\]
While accurate for some algorithms and on very simple architectures, this model generally breaks down due to a lot of factors: for instance, the communication pattern of the application, the bandwith of the interconnection network and, more in general, due to the complexity of the parallel architectures. Despite its weakness, for practical reasons we will refer to the $T\_send$-model to explain the results we achieved.  
The following experimental analysis will try to estimate $T\_send$ on our test architectures by varying the size of the messages and by changing the communication pattern (we will perform the so called ''bisection test'', whichin there are two sets of processes, with same cardinality, that exchange messages each other). 

In order to benchmark the performance of the two environments, we decided to use the \textit{Perftest} benchmark suite.

The perftest-package is provided along with the MPI-implementation MPICH, although it can be used in combination with any MPI-implementation. The package contains a few tools to measure the performance of a message passing environment. The two major programs are \textit{mpptest} for measuring point-to-point communication and \textit{goptest} for measuring collective communication. In addition to the classic ping-pong test, mpptest can measure performance with many participating processes (exposing contention and scalability problems) and can adaptively choose the message sizes in order to isolate sudden changes in performance. 

The following test were performed:
\begin{itemize}
	\item Round-trip times between 2 nodes using blocking methods
	\item Blocking bisection times involving from 4 to $n$ nodes
	\item Broadcast times involving from 2 to $n$ nodes
\end{itemize}
In the bisection test the complete system is logically divided into two subsystems and the aggregated bandwidth and latency between the two subsystems is measured. An example of this is splitting the system in two vertically and letting each node in the left half communicate with a node in the right part on a one-to-one basis.

\paragraph{Optimum size of the DAL's communication buffer}
As we said in section~\ref{DAL-impl} we are interested in estimating the \textit{optimum size} for the communication buffer of the DAL layer. In particular, let's assume that we are performing a $DAL\_send$ of $L$ words. We may have to send these $L$ words either by performing a single $MPI\_Send$ or, more in general, by performing $X$ $MPI\_Send$. The latter situation is not unusual in our framework due to the structure of the DAL layer. Indeed, from section~\ref{DAL-impl}, we remind that each process has its own buffer dedicated to MPI-communications, and this buffer is of fixed size, let's say $K$ words. Since in general $K < L$, assuming without loss of generality that $L = K \times X$, the cost of sending $L$ words is equal to the cost of performing $X$ $MPI\_Send$ each of size $K$. For each architecture, we have to choose the best value of $K$: indeed, a low value may negatively impact on $T\_send$ (because of the setup cost of each $MPI\_Send$); a high value could be useless because a too large communication buffer may not give any benefit, rather it may only constitue a waste of important memory. In order to find the optimal value of $K$, namely the size of the ''static'' communication buffer, we developed a set of utilites (program and script that goes under the name of \textit{Tsetup}) that practically estimates $T\_send$ as function of $L$ and $X$. 

\paragraph{MPI on Pianosa}
\label{test-env-pianosa}
Figures~\ref{pianosa-mpi-1},~\ref{pianosa-mpi-2},~\ref{pianosa-mpi-3} show the results obtained on Pianosa.

\begin{figure}[p]
	\centering
  	\subfloat[Messages of length from 0 to 32 bytes (stride 4 bytes).]{\label{tsend_32}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/tsend_4}}  
	\hspace*{20pt}
  	\subfloat[Messages of length from 0 to 1024 bytes (stride 32 bytes).]{\label{tsend_32}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/tsend_32}}  
	\caption{Performance of blocking send between two processes.}
	\label{pianosa-mpi-1}
	
	\centering
  	\subfloat[Messages of length from 0 to 32768 bytes (stride $2^i$ bytes).]{\label{tsend_logscale}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/tsend_logscale}}  
	\hspace*{20pt}
  	\subfloat[Bisection test with 4, 8 and 16 processors.]{\label{bisect_logscale}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/bisect_logscale}}  
	\caption{Performance of blocking send and bisection test.}
	\label{pianosa-mpi-2}	
	
	\centering {\label{bcast}\includegraphics[width=0.4\textwidth]{../tests/mpi_comm_perf/pianosa/bcast}}  
	\caption{Performance of broadcast with 4, 8 and 16 processors.}
	\label{pianosa-mpi-3}
\end{figure}

%Table~\ref{tsetup-impact} shows the $T\_send$ for sending a message of size 512MB. Each row of the table shows the $T\_send$ that comes from splitting the original message into $X$ $MPI\_Send$, with $X \in \lbrace 2^i : i = 0...19 \rbrace$ (so, in the worst case leads to perform rougly 500000 $MPI\_Send$ of size 1KB). We can evince the impact of $T\_setup$ on $T\_send$ by analyzing the growing of $T\_send$ (that is, the elapsed time for sending a message). Analagously, table~\ref{tsetup-impact-16} refers to messages of size 16MB. TODO:comment
%\begin{table}[h]
%\begin{center}
%\begin{tabular}{|c|c|c|}\hline
%\hline
%$\sharp$Sends          & $MPI\_Send$ size ($\frac{L}{\sharp Sends}$ bytes)          & $T\_send$ (Elapsed time, sec)      \\\hline\hline
%1 & 536870912 & 46.45626 \\\hline
%2 & 268435456 & 46.45627 \\\hline
%4 & 134217728 & 46.45626 \\\hline
%8 & 67108864 & 46.45633 \\\hline
%16 & 33554432 & 46.45642 \\\hline
%32 & 16777216 & 46.45661 \\\hline
%64 & 8388608 & 46.45693 \\\hline
%128 & 4194304 & 46.45716 \\\hline
%256 & 2097152 & 46.45910 \\\hline
%512 & 1048576 & 46.45994 \\\hline
%1024 & 524288 & 47.46822 \\\hline
%2048 & 262144 & 48.48218 \\\hline
%4096 & 131072 & 46.45640 \\\hline
%8192 & 65536 & 46.45656 \\\hline
%16384 & 32768 & 46.45672 \\\hline
%32768 & 16384 & 46.45721 \\\hline
%65536 & 8192 & 46.45806 \\\hline
%131072 & 4096 & 46.45985 \\\hline
%262144 & 2048 & 46.46339 \\\hline
%524288 & 1024 & 47.47053 \\\hline
%\hline
%\end{tabular}
%\caption{$T\_send$ for messages of size 512MB. }
%\label{tsetup-impact}
%\end{center}
%\end{table}
%
%\begin{table}[h]
%\begin{center}
%\begin{tabular}{|c|c|c|}\hline
%\hline
%$\sharp$Sends          & $MPI\_Send$ size ($\frac{L}{\sharp Sends}$ bytes)          & $T\_send$ (Elapsed time, sec)      \\\hline\hline
%1 & 16777216 & 1.1421 \\\hline
%2 & 8388608 & 1.1421 \\\hline
%4 & 4194304 & 1.1424 \\\hline
%8 & 2097152 & 1.1427 \\\hline
%16 & 1048576 & 1.1430 \\\hline
%32 & 524288 & 1.1455 \\\hline
%64 & 262144 & 1.1500 \\\hline
%128 & 131072 & 1.1421 \\\hline
%256 & 65536 & 1.1420 \\\hline
%512 & 32768 & 1.1423 \\\hline
%1024 & 16384 & 1.1423 \\\hline
%2048 & 8192 & 1.1426 \\\hline
%4096 & 4096 & 1.1431 \\\hline
%8192 & 2048 & 1.1443 \\\hline
%16384 & 1024 & 1.1467 \\\hline
%32768 & 512 & 2.1512 \\\hline
%65536 & 256 & 2.1639 \\\hline
%131072 & 128 & 3.2541 \\\hline
%262144 & 64 & 3.2842 \\\hline
%524288 & 32 & 6.5541 \\\hline
%\hline
%\end{tabular}
%\caption{$T\_send$ for messages of size 16MB. }
%\label{tsetup-impact-16}
%\end{center}
%\end{table}

\paragraph{MPI on Physics}

\input{performance}

\subsection{Algorithms Efficiency}
In this section, we are going to analyze the theoretical Algorithm Efficiency already defined in~\ref{terminology}. For the sake of commodity, we report here the definition. Given
\begin{itemize}
\item $P$, the number of real processors of our machine; 
\item $m$, the number of steps of the algorithm;
\item $X_i$, a random variable which counts the number of real processors that contribute to the calculation during the step $i$.
\end{itemize}
We defined the Algorithm Efficiency $\varphi$ as:
\begin{center}
$\varphi = \frac{\sum_{i=0}^m E[X_i]}{P \times m} $
\end{center}
TODO: for each algorithm, give the efficiency.

\subsection{Comparing the results}