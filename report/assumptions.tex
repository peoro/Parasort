Now we will summarize assumptions and design choices that we made for this project, emerged during the several meetings. 

\paragraph{Common framework} The work we need to do is to compare parallel sorting algorithms and to study how they behave. We early understood that to achieve such goal we were forced to collaborate in some way. We had to define a common framework in order to have \textit{comparable results}. If any of us computed times in different ways, these wouldn't have been comparable. If any of us used a different strategy to sort data sequentially, or different optimization to sort data in principal memory or in disk, results would have been worthless, since the comparison would have mixed the performance of a parallel sorting algorithm with the performance of something else that regards sequential computation. Another fundamental reason comes directly from the \textit{complexity of the implementation}. For instance, the main problem we encountered, as we will explain in~\ref{DAL}, was the handling of large data sets. Managing gigabytes or terabytes of datas means that our algorithms cannot rely upon only the primary memory; there must be some kind of interaction with files on disks, for what concerning both the sorting and the communication between processes. We will see that sharing a common framework is fundamental for keeping limited the complexity of parallel sorting algorithms which cannot neglect the possibility of sorting large data sets. This approach, besides, allowed us to do a better work, having a well structured framework which can be extended to many other algorithms with little work.

\paragraph{Logic of parallel algorithms} We are not interested to study how \textit{sequential} algorithms behave. We care only about the parallel part of our algorithms, that is how the processes of a parallel algorithms interact each other. The sequential part, that is the sorting of a local partition of datas, is based on a well-known standard algorithm for large data sets, which is the \textit{multi-way mergesort}. Sequential and parallel sorting are completely disjoint operations, which can be studied separately.

\paragraph{Centralization of data sets} Some algorithms natively begin, or end, with data centralized in a single process, while others begin or end with data distributed among all processes. For example Mergesort begins with data distributed: any process sorts its piece of data, and send it to another node that takes care of merging it, in order to end with data centralized on a single process. Quicksort, at the opposite, start with centralized data; the first process splits data and sends it to the other, in order to end up with distributed sorted data. For this reason and to maintain some homogeneity among the algorithms we chose to always begin and end with centralized data. The algorithms that begin or end with distributed data will take care of scattering or gathering data, studying these extra (ie: unneeded by the algorithm, but needed by our common algorithm-testbed) operations separately from the rest of the computation.

\paragraph{Stencils} Since our platform is a cluster made of multi-core nodes, we know that communication patterns between our several processes may weight a lot on the final performances. We will be looking for heuristic approaches to map processes in order to study what scenario will result in best performances due to communications between cores on different nodes, or on different parts of the single node.

